"""
Backend Service - Version robuste sans points d'√©chec
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

from pydantic import BaseModel
from typing import List, Dict, Optional
import os
import requests
from datetime import datetime
from loguru import logger
from importlib import metadata
from app.business_prompts import get_business_prompt, get_available_business_types, get_business_type_display_name

# Import OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.error("OpenAI package not available")

app = FastAPI(title="Backend Intelligence Service", description="Rapports longs cabinet de conseil - version robuste")

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
VECTOR_SERVICE_URL = "http://vector-service:8002"

# Mod√®les Pydantic
class BusinessAnalysisRequest(BaseModel):
    business_type: str
    analysis_type: str
    query: str
    title: Optional[str] = None

class AnalysisResponse(BaseModel):
    analysis_type: str
    business_type: str
    title: str
    content: str
    sources: List[Dict]
    metadata: Dict
    timestamp: str

class ChatRequest(BaseModel):
    message: str
    business_type: Optional[str] = None
    conversation_history: Optional[List[Dict]] = []

class ChatResponse(BaseModel):
    response: str
    business_context: str
    sources: List[Dict]
    metadata: Dict
    timestamp: str

def search_documents_safe(query: str, top_k: int = 10) -> List[Dict]:
    """Recherche vectorielle avec gestion d'erreurs robuste"""
    try:
        response = requests.post(
            f"{VECTOR_SERVICE_URL}/search",
            json={"query": query, "top_k": top_k},
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            # The vector-service returns a LIST of results. Also support dict forms.
            if isinstance(result, list):
                return result
            if isinstance(result, dict):
                return result.get("results", result.get("data", []))
            return []
        else:
            logger.warning(f"Vector search failed: {response.status_code}")
            return []
            
    except requests.exceptions.Timeout:
        logger.error("Vector search timeout")
        return []
    except requests.exceptions.ConnectionError:
        logger.error("Vector search connection error")
        return []
    except Exception as e:
        logger.error(f"Vector search error: {e}")
        return []

def enrich_source_with_apa(doc: Dict, index: int) -> Dict:
    """Enrichit une source avec m√©tadonn√©es APA pour citations acad√©miques"""
    doc_id = doc.get("doc_id", "N/A")
    text = str(doc.get("text", ""))
    score = doc.get("score", 0)
    
    # Extraction m√©tadonn√©es intelligentes (√† am√©liorer avec vraies m√©tadonn√©es)
    # Pour l'instant, g√©n√©ration bas√©e sur doc_id
    year = 2024
    author = "Axial Research"
    title = f"Document d'analyse strat√©gique #{doc_id}"
    page = (doc_id % 50) + 1 if isinstance(doc_id, int) else 1
    
    # D√©termine le type de document bas√© sur le contenu
    if "march√©" in text.lower() or "market" in text.lower():
        doc_type = "Rapport de march√©"
        author = "Axial Market Intelligence"
    elif "tech" in text.lower() or "digital" in text.lower():
        doc_type = "Veille technologique"
        author = "Axial Tech Watch"
    elif "risque" in text.lower() or "risk" in text.lower():
        doc_type = "Analyse de risques"
        author = "Axial Risk Assessment"
    else:
        doc_type = "Document interne"
    
    # Format APA: Auteur. (Ann√©e). Titre. Type, p. page.
    apa_citation = f"{author}. ({year}). {title}. {doc_type}, p. {page}."
    
    return {
        "id": index,
        "doc_id": doc_id,
        "title": title,
        "author": author,
        "year": year,
        "page": page,
        "doc_type": doc_type,
        "text": text[:300],  # Preview plus long
        "score": score,
        "apa_citation": apa_citation,
        "document_url": f"/documents/{doc_id}.pdf" if doc_id != "N/A" else None
    }

def format_context_safe(documents: List[Dict]) -> str:
    """Formate contexte de mani√®re s√©curis√©e"""
    if not documents:
        return "Aucun document de r√©f√©rence disponible."
    
    context = "## DOCUMENTS DE R√âF√âRENCE\n\n"
    for i, doc in enumerate(documents[:6], 1):  # Limiter √† 6 docs
        try:
            doc_text = str(doc.get('text', ''))[:500]  # Limiter texte
            score = float(doc.get('score', 0))
            doc_id = str(doc.get('doc_id', 'N/A'))
            context += f"**[R√©f. {i}]** (Score: {score:.3f}):\n{doc_text}...\n\n"
        except Exception as e:
            logger.warning(f"Error formatting document {i}: {e}")
            continue
    
    return context

def create_optimized_prompt(business_type: str, analysis_type: str, query: str, context: str) -> str:
    """Cr√©e prompts ultra-structur√©s pour rapports de cabinet de conseil"""
    
    # Templates ultra-d√©taill√©s avec citations APA
    prompt_templates = {
        "finance_banque": f"""üìä ANALYSE STRAT√âGIQUE BANCAIRE - FORMAT CABINET DE CONSEIL

üéØ MISSION: {query}

üìö CONTEXTE DOCUMENTAIRE:
{context[:5000]}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

INSTRUCTIONS DE R√âDACTION (FORMAT CABINET CONSEIL):

1. CITATIONS ACAD√âMIQUES OBLIGATOIRES:
   - Format: [¬π], [¬≤], [¬≥] pour citations inline
   - CHAQUE donn√©e chiffr√©e DOIT avoir sa citation
   - CHAQUE affirmation factuelle DOIT √™tre sourc√©e
   - Exemple: "Le march√© bancaire cro√Æt de 3% [¬π]"

2. STRUCTURE ULTRA-D√âTAILL√âE REQUISE:

# üìã RAPPORT STRAT√âGIQUE BANCAIRE

## üéØ EXECUTIVE SUMMARY (1-2 pages)
### Contexte et Enjeux Strat√©giques
- Situation actuelle du secteur avec donn√©es [¬π]
- Enjeux de transformation majeurs [¬≤]
- Opportunit√©s et menaces imm√©diates [¬≥]

### Recommandations Prioritaires
1. **Action Priorit√© 1**: [Description d√©taill√©e] - ROI estim√©, timeline
2. **Action Priorit√© 2**: [Description d√©taill√©e] - ROI estim√©, timeline  
3. **Action Priorit√© 3**: [Description d√©taill√©e] - ROI estim√©, timeline

### Impact Business Attendu
- KPIs quantifi√©s avec benchmarks sectoriels [¬π]
- Timeline de mise en ≈ìuvre (6-12-18 mois)
- Budget et ressources n√©cessaires

---

## üìä ANALYSE SECTORIELLE APPROFONDIE (3-4 pages)

### 1. Dimensionnement du March√©
- **Taille actuelle**: XX M‚Ç¨/M$ [¬π]
- **Croissance annuelle**: XX% [¬≤]
- **Pr√©visions 2025-2030**: D√©taill√©es avec hypoth√®ses [¬≥]
- **Parts de march√©**: Top 10 acteurs avec √©volution [‚Å¥]

### 2. Segmentation et Dynamiques
- **Segments de client√®le**: Retail, Corporate, Private Banking [¬π]
- **√âvolution comportements clients**: Digitalisation, attentes [¬≤]
- **Produits/Services porteurs**: Analyse d√©taill√©e [¬≥]

### 3. Technologies et Innovation
- **Fintech et disruption**: Impact sur acteurs traditionnels [¬π]
- **IA et automatisation**: Cas d'usage bancaires [¬≤]
- **Blockchain et crypto**: Opportunit√©s et risques [¬≥]
- **Open Banking**: √âtat des lieux r√©glementaire [‚Å¥]

### 4. Environnement R√©glementaire
- **Contraintes majeures**: B√¢le III/IV, MiFID II, etc. [¬π]
- **Impact op√©rationnel**: Co√ªts compliance, reporting [¬≤]
- **√âvolutions √† venir**: Anticipation 2025-2026 [¬≥]

---

## ‚öîÔ∏è ANALYSE CONCURRENTIELLE (2-3 pages)

### Mapping Concurrentiel
**Quadrant Leaders (Market Leaders)**:
- Acteur A: Forces [¬π], Faiblesses [¬≤], Parts de march√© XX% [¬≥]
- Acteur B: Forces [¬π], Faiblesses [¬≤], Parts de march√© XX% [¬≥]

**Quadrant Challengers**:
- [Analyse d√©taill√©e avec donn√©es chiffr√©es]

**Quadrant Niche Players**:
- [Analyse d√©taill√©e avec positionnement]

### Strat√©gies de Diff√©renciation
1. **Par l'innovation**: Exemples concrets [¬π]
2. **Par l'exp√©rience client**: Benchmarks NPS [¬≤]
3. **Par les co√ªts**: Efficiency ratio compar√©s [¬≥]

### Menaces Comp√©titives
- **Nouveaux entrants**: Fintechs, BigTech [¬π]
- **Substituts**: Monnaies digitales, DeFi [¬≤]
- **Consolidation**: M&A r√©centes et √† venir [¬≥]

---

## üí° RECOMMANDATIONS STRAT√âGIQUES (3-4 pages)

### Plan d'Action Imm√©diat (0-6 mois)
**Initiative 1: [Titre]**
- Objectif: [D√©taill√© et quantifi√©]
- Actions: [Liste num√©rot√©e avec responsables]
- ROI: XX% ou XX M‚Ç¨ [¬π]
- Risques: [Identifi√©s avec mitigation]
- KPIs: [3-5 indicateurs mesurables]

**Initiative 2: [Titre]**
[M√™me structure d√©taill√©e]

### Plan d'Action Moyen Terme (6-18 mois)
[3-4 initiatives structur√©es identiquement]

### Investissements Requis
| Poste | Budget | Timeline | ROI Attendu |
|-------|--------|----------|-------------|
| IT/Digital | XX M‚Ç¨ | Q1-Q4 | XX% [¬π] |
| Talents | XX M‚Ç¨ | Continu | XX% [¬≤] |
| Marketing | XX M‚Ç¨ | Q2-Q3 | XX% [¬≥] |

---

## üìà PROJECTIONS ET SC√âNARIOS (2 pages)

### Sc√©nario Optimiste (+15% croissance)
- Hypoth√®ses: [List√©es et sourc√©es]
- Impacts business: [Quantifi√©s] [¬π]
- Probabilit√©: XX% bas√©e sur [¬≤]

### Sc√©nario Central (+8% croissance)
[M√™me structure]

### Sc√©nario Pessimiste (+2% croissance)
[M√™me structure]

### KPIs de Suivi Recommand√©s
1. **Revenue Growth**: Target XX% [¬π]
2. **Market Share**: Target XX% [¬≤]
3. **Cost/Income Ratio**: Target XX% [¬≥]
4. **NPS Client**: Target XX/100 [‚Å¥]
5. **Digital Adoption**: Target XX% [‚Åµ]

---

## üìö BIBLIOGRAPHIE APA

[1] Auteur. (Ann√©e). Titre document. Type, p. XX.
[2] Auteur. (Ann√©e). Titre document. Type, p. XX.
[...] [Toutes les sources cit√©es]

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

EXIGENCES QUALIT√â:
‚úÖ Minimum 6000 mots (format cabinet conseil)
‚úÖ TOUTES les donn√©es chiffr√©es cit√©es [¬π][¬≤][¬≥]
‚úÖ Espacement markdown clair (lignes vides entre sections)
‚úÖ Tableaux pour donn√©es comparatives
‚úÖ Listes √† puces pour lisibilit√©
‚úÖ Bibliographie APA compl√®te en fin

G√âN√àRE MAINTENANT CE RAPPORT ULTRA-D√âTAILL√â:""",

        "tech_digital": f"""ANALYSE TRANSFORMATION DIGITALE

MISSION: {query}

CONTEXTE:
{context[:4000]}

G√âN√àRE RAPPORT TECHNIQUE STRAT√âGIQUE (10+ pages):

# RAPPORT TRANSFORMATION DIGITALE

## üéØ VISION EX√âCUTIVE
- Enjeux transformation [R√©f. X]
- ROI digital [R√©f. X]
- Roadmap strat√©gique

## üîß √âTAT DES LIEUX TECH
- Maturit√© technologique [R√©f. X]
- Gaps et opportunit√©s [R√©f. X]
- Benchmark secteur [R√©f. X]

## üöÄ INNOVATION
- Technologies cl√©s [R√©f. X]
- Use cases business [R√©f. X]
- Investissements [R√©f. X]

## üìã PLAN D'ACTION
- Phases transformation [R√©f. X]
- Budget et timeline [R√©f. X]
- Organisation et skills [R√©f. X]

Minimum 5000 mots. R√©f√©rencer [R√©f. X] syst√©matiquement.""",

        "retail_commerce": f"""ANALYSE RETAIL STRAT√âGIQUE

MISSION: {query}

CONTEXTE:
{context[:4000]}

G√âN√àRE RAPPORT RETAIL COMPLET (10+ pages):

# RAPPORT STRAT√âGIE RETAIL

## üéØ SYNTH√àSE RETAIL
- Tendances march√© [R√©f. X]
- Transformation omnicanal [R√©f. X]
- Strat√©gies gagnantes

## üõçÔ∏è MARCH√â ET CLIENTS
- √âvolution consommation [R√©f. X]
- Segments clients [R√©f. X]
- Parcours d'achat [R√©f. X]

## üè™ CONCURRENCE
- Players traditionnels vs pure players [R√©f. X]
- Innovations retail [R√©f. X]
- Diff√©renciation [R√©f. X]

## üí° RECOMMANDATIONS
- Strat√©gie omnicanal [R√©f. X]
- Technologies retail [R√©f. X]
- Plan d√©ploiement [R√©f. X]

Minimum 5000 mots. Citer [R√©f. X] pour donn√©es factuelles."""
    }
    
    return prompt_templates.get(business_type, prompt_templates["finance_banque"])

def call_openai_safe(prompt: str, business_type: str) -> str:
    """Appel OpenAI s√©curis√© avec gestion d'erreurs compl√®te"""
    try:
        if not OPENAI_API_KEY or OPENAI_API_KEY == "":
            return "‚ö†Ô∏è **Configuration OpenAI requise**\n\nVeuillez configurer la variable OPENAI_API_KEY dans votre fichier .env"
        
        # V√©rifier OpenAI
        if not OPENAI_AVAILABLE:
            return "‚ùå **Module OpenAI manquant**\n\nVeuillez installer: pip install openai"
        
        # System prompts par m√©tier
        system_prompts = {
            "finance_banque": """Tu es un consultant senior McKinsey sp√©cialis√© en strat√©gie bancaire. 
                              G√©n√®re des rapports structur√©s avec analyses quantifi√©es et recommandations actionnables.
                              Utilise exclusivement les donn√©es des documents fournis avec r√©f√©rences [R√©f. X].""",
                              
            "tech_digital": """Tu es un consultant BCG expert en transformation digitale. 
                             G√©n√®re des analyses techniques d√©taill√©es avec business case et ROI.
                             Base tes analyses sur les documents fournis avec r√©f√©rences [R√©f. X].""",
                             
            "retail_commerce": """Tu es un consultant Bain expert en retail et commerce. 
                                G√©n√®re des analyses avec insights consommateurs et recommandations op√©rationnelles.
                                Utilise les documents fournis avec r√©f√©rences [R√©f. X]."""
        }
        
        system_prompt = system_prompts.get(business_type, system_prompts["finance_banque"])
        
        # Client OpenAI avec param√®tres explicites
        try:
            client = OpenAI(
                api_key=OPENAI_API_KEY,
                timeout=300.0  # 5 minutes max pour rapports longs
            )
            
            # V√©rifier taille prompt
            if len(prompt) > 15000:
                logger.warning(f"Prompt tr√®s long ({len(prompt)} chars), troncature appliqu√©e")
                prompt = prompt[:15000] + "\n\n[...Prompt tronqu√© pour limites techniques. Continuer l'analyse avec les √©l√©ments disponibles...]"
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=8000
            )
            
            return response.choices[0].message.content
            
        except Exception as api_error:
            logger.error(f"OpenAI API error: {api_error}")
            return f"‚ùå **Erreur API OpenAI**\n\n{str(api_error)[:300]}\n\nV√©rifiez votre cl√© API et votre quota."
        
    except Exception as e:
        logger.error(f"Critical error in OpenAI call: {e}")
        return f"‚ùå **Erreur critique**\n\n{str(e)[:300]}"

async def generate_business_analysis_safe(business_type: str, analysis_type: str, query: str, title: str = None) -> AnalysisResponse:
    """G√©n√®re analyse avec gestion d'erreurs compl√®te"""
    try:
        logger.info(f"Starting analysis: {business_type}/{analysis_type}")
        
        # 1. Recherche documents s√©curis√©e
        documents = search_documents_safe(query, top_k=8)
        logger.info(f"Found {len(documents)} documents")
        
        # 2. Formatage contexte s√©curis√©
        context = format_context_safe(documents)
        
        # 3. Cr√©ation prompt optimis√©
        prompt = create_optimized_prompt(business_type, analysis_type, query, context)
        
        # 4. Appel OpenAI s√©curis√©
        content = call_openai_safe(prompt, business_type)
        
        # 5. Construction r√©ponse avec sources enrichies APA
        enriched_sources = [enrich_source_with_apa(d, i+1) for i, d in enumerate(documents)]
        
        return AnalysisResponse(
            analysis_type=analysis_type,
            business_type=business_type,
            title=title or f"Rapport {get_business_type_display_name(business_type)} - {analysis_type.replace('_', ' ').title()}",
            content=content,
            sources=enriched_sources,
            metadata={
                "query": query,
                "business_type": business_type,
                "documents_found": len(documents),
                "analysis_length": "extended_report",
                "model": "gpt-4o-mini",
                "max_tokens": 8000,
                "status": "success",
                "citation_format": "APA"
            },
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Error in business analysis: {e}")
        # Retourner une r√©ponse d'erreur plut√¥t qu'une exception
        return AnalysisResponse(
            analysis_type=analysis_type,
            business_type=business_type,
            title=title or "Analyse √©chou√©e",
            content=f"‚ùå **Erreur lors de l'analyse**\n\n{str(e)[:500]}\n\nVeuillez r√©essayer ou contacter le support.",
            sources=[],
            metadata={
                "query": query,
                "business_type": business_type,
                "error": str(e),
                "status": "failed"
            },
            timestamp=datetime.now().isoformat()
        )

async def generate_chat_response_safe(message: str, business_type: str = None, history: List[Dict] = None) -> ChatResponse:
    """Chat avec gestion d'erreurs robuste"""
    try:
        # 1. Recherche documents
        documents = search_documents_safe(message, top_k=5)
        context = format_context_safe(documents)
        
        # 2. Construction prompt chat
        business_context = get_business_type_display_name(business_type) if business_type else "G√©n√©raliste"
        
        chat_prompt = f"""Tu es un assistant expert sp√©cialis√© {business_context}.

CONTEXTE DOCUMENTAIRE:
{context}

HISTORIQUE CONVERSATION:
{history[-3:] if history else "Nouvelle conversation"}

QUESTION: {message}

INSTRUCTIONS DE CITATION (FORMAT ACAD√âMIQUE):
- Utilise les citations inline avec num√©ros exposants: [¬π], [¬≤], [¬≥], etc.
- TOUJOURS citer la source imm√©diatement apr√®s chaque information factuelle
- En fin de r√©ponse, ajoute une section "## üìö Sources" avec bibliographie APA compl√®te
- Exemple de citation inline: "Le march√© cro√Æt de 3% [¬π]"
- Format bibliographie: [1] Auteur. (Ann√©e). Titre. Type, p. page.

STRUCTURE DE R√âPONSE REQUISE:
1. R√©ponse concise et professionnelle (2-3 paragraphes max)
2. Informations factuelles avec citations [¬π][¬≤][¬≥]
3. Section "## üìö Sources" en fin avec r√©f√©rences APA

R√©ponds de mani√®re structur√©e et professionnelle en te basant sur les documents fournis.
"""

        # 3. Appel OpenAI s√©curis√©
        response_content = call_openai_safe(chat_prompt, business_type or "finance_banque")
        
        # Enrichir sources avec APA
        enriched_sources = [enrich_source_with_apa(d, i+1) for i, d in enumerate(documents[:5])]
        
        return ChatResponse(
            response=response_content,
            business_context=business_context,
            sources=enriched_sources,
            metadata={
                "message": message,
                "business_type": business_type,
                "documents_found": len(documents),
                "model": "gpt-4o-mini",
                "citation_format": "APA"
            },
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Error in chat response: {e}")
        return ChatResponse(
            response=f"‚ùå Erreur dans la r√©ponse: {str(e)[:200]}",
            business_context=business_type or "Error",
            sources=[],
            metadata={"error": str(e)},
            timestamp=datetime.now().isoformat()
        )

# Endpoints
@app.get("/health")
def health():
    """Health check avec diagnostics √©tendus"""
    return {
        "status": "healthy", 
        "service": "backend-intelligence-fixed",
        "openai_configured": bool(OPENAI_API_KEY),
        "vector_service": VECTOR_SERVICE_URL,
        "business_types": get_available_business_types(),
        "version": "1.0-robust"
    }

@app.get("/business-types")
def get_business_types():
    """Types de m√©tier disponibles"""
    try:
        return {
            "business_types": [
                {"key": bt, "display_name": get_business_type_display_name(bt)} 
                for bt in get_available_business_types()
            ]
        }
    except Exception as e:
        logger.error(f"Error getting business types: {e}")
        return {
            "business_types": [
                {"key": "finance_banque", "display_name": "üè¶ Finance & Banque"},
                {"key": "tech_digital", "display_name": "üíª Tech & Digital"},
                {"key": "retail_commerce", "display_name": "üõçÔ∏è Retail & Commerce"}
            ]
        }

@app.post("/extended-analysis", response_model=AnalysisResponse)
async def extended_analysis(request: BusinessAnalysisRequest):
    """G√©n√®re rapports longs style cabinet conseil - Version robuste"""
    return await generate_business_analysis_safe(
        request.business_type,
        request.analysis_type,
        request.query,
        request.title
    )

@app.post("/business-analysis", response_model=AnalysisResponse)
async def business_analysis(request: BusinessAnalysisRequest):
    """Alias pour compatibilit√©"""
    return await generate_business_analysis_safe(
        request.business_type,
        request.analysis_type,
        request.query,
        request.title
    )

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Chat intelligent avec contexte m√©tier"""
    return await generate_chat_response_safe(
        request.message,
        request.business_type,
        request.conversation_history
    )

@app.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """Streaming de la r√©ponse du chat (texte brut chunk√©, compatible fetch streaming)."""
    async def token_generator():
        try:
            # 1) RAG rapide (synchrone)
            documents = search_documents_safe(request.message, top_k=5)
            context = format_context_safe(documents)
            business_context = get_business_type_display_name(request.business_type) if request.business_type else "G√©n√©raliste"
            chat_prompt = f"""Tu es un assistant expert sp√©cialis√© {business_context}.

CONTEXTE DOCUMENTAIRE:
{context}

HISTORIQUE CONVERSATION:
{request.conversation_history[-3:] if request.conversation_history else "Nouvelle conversation"}

QUESTION: {request.message}

R√©ponds de mani√®re concise et professionnelle en te basant sur les documents fournis.
Si la question d√©passe ton domaine d'expertise, oriente vers le bon sp√©cialiste.
Cite [R√©f. X] pour les informations factuelles.
"""

            # 2) Streaming OpenAI
            if not OPENAI_API_KEY or not OPENAI_AVAILABLE:
                # Fallback non‚Äëbloquant
                yield "Le streaming n√©cessite une configuration OPENAI_API_KEY.\n"
                yield "[DONE]"
                return

            client = OpenAI(api_key=OPENAI_API_KEY, timeout=300.0)
            stream = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"Assistant sp√©cialis√© {business_context}."},
                    {"role": "user", "content": chat_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
                stream=True,
            )

            for event in stream:
                try:
                    delta = event.choices[0].delta if hasattr(event.choices[0], "delta") else event.choices[0].get("delta", {})
                    content = getattr(delta, "content", None)
                    if content is None and isinstance(delta, dict):
                        content = delta.get("content")
                    if content:
                        yield content
                except Exception as inner:
                    logger.warning(f"Stream delta parse error: {inner}")
                    continue

            yield "[DONE]"
        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield "\n[STREAM_ERROR]"

    headers = {
        "Cache-Control": "no-cache",
        "X-Accel-Buffering": "no",  # Nginx: disable buffering
    }
    return StreamingResponse(token_generator(), media_type="text/plain", headers=headers)

@app.get("/test-openai")
async def test_openai():
    """Test de connectivit√© OpenAI"""
    try:
        if not OPENAI_API_KEY:
            return {"status": "error", "message": "OPENAI_API_KEY not configured"}
        
        client = OpenAI(
            api_key=OPENAI_API_KEY,
            timeout=300.0  # 5 minutes max
        )
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "Hello, test simple"}],
            max_tokens=10
        )
        
        return {
            "status": "success", 
            "message": "OpenAI API functional",
            "model": "gpt-4o-mini",
            "response": response.choices[0].message.content
        }
        
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/diagnostics")
async def diagnostics():
    """Diagnostics complets du syst√®me"""
    
    diagnostics_result = {
        "timestamp": datetime.now().isoformat(),
        "service": "backend-intelligence-fixed",
        "version": "1.0-robust"
    }
    
    # Versions des libs cl√©s
    try:
        diagnostics_result["versions"] = {
            "python": os.getenv("PYTHON_VERSION", "unknown"),
            "openai": metadata.version("openai") if OPENAI_AVAILABLE else "not-installed",
            "httpx": metadata.version("httpx") if "httpx" in {d.metadata["Name"].lower() for d in map(lambda n: metadata.distribution(n), metadata.packages_distributions().keys()) if False} else metadata.version("httpx")
        }
    except Exception:
        try:
            diagnostics_result["versions"] = {
                "openai": metadata.version("openai") if OPENAI_AVAILABLE else "not-installed",
                "httpx": metadata.version("httpx")
            }
        except Exception as e:
            diagnostics_result["versions_error"] = str(e)

    # Proxies d'environnement (pour debug)
    diagnostics_result["env_proxies"] = {
        k: os.getenv(k)
        for k in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"]
        if os.getenv(k)
    }

    # Test OpenAI
    try:
        if OPENAI_API_KEY:
            client = OpenAI(
                api_key=OPENAI_API_KEY,
                timeout=300.0  # 5 minutes max
            )
            test_response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            diagnostics_result["openai"] = {"status": "‚úÖ Functional", "model": "gpt-4o-mini"}
        else:
            diagnostics_result["openai"] = {"status": "‚ùå Not configured", "model": None}
    except Exception as e:
        diagnostics_result["openai"] = {"status": f"‚ùå Error: {str(e)[:100]}", "model": "gpt-4o-mini"}
    
    # Test Vector Service
    try:
        test_docs = search_documents_safe("test", top_k=1)
        diagnostics_result["vector_service"] = {
            "status": "‚úÖ Accessible" if len(test_docs) >= 0 else "‚ö†Ô∏è No results",
            "url": VECTOR_SERVICE_URL,
            "test_results": len(test_docs)
        }
    except Exception as e:
        diagnostics_result["vector_service"] = {"status": f"‚ùå Error: {str(e)[:100]}", "url": VECTOR_SERVICE_URL}
    
    # Test Business Types
    try:
        business_types = get_available_business_types()
        diagnostics_result["business_types"] = {"status": "‚úÖ Available", "count": len(business_types), "types": business_types}
    except Exception as e:
        diagnostics_result["business_types"] = {"status": f"‚ùå Error: {str(e)[:100]}"}
    
    return diagnostics_result

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8006)