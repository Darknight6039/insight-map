"""
Backend Service - Version robuste sans points d'√©chec
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

from pydantic import BaseModel
from typing import List, Dict, Optional
import os
import requests
from datetime import datetime
from loguru import logger
from importlib import metadata
from app.business_prompts import get_business_prompt, get_available_business_types, get_business_type_display_name

# Import OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.error("OpenAI package not available")

app = FastAPI(title="Backend Intelligence Service", description="Rapports longs cabinet de conseil - version robuste")

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
VECTOR_SERVICE_URL = "http://vector-service:8002"

# Mod√®les Pydantic
class BusinessAnalysisRequest(BaseModel):
    business_type: str
    analysis_type: str
    query: str
    title: Optional[str] = None

class AnalysisResponse(BaseModel):
    analysis_type: str
    business_type: str
    title: str
    content: str
    sources: List[Dict]
    metadata: Dict
    timestamp: str

class ChatRequest(BaseModel):
    message: str
    business_type: Optional[str] = None
    conversation_history: Optional[List[Dict]] = []

class ChatResponse(BaseModel):
    response: str
    business_context: str
    sources: List[Dict]
    metadata: Dict
    timestamp: str

def search_documents_safe(query: str, top_k: int = 10) -> List[Dict]:
    """Recherche vectorielle avec gestion d'erreurs robuste"""
    try:
        response = requests.post(
            f"{VECTOR_SERVICE_URL}/search",
            json={"query": query, "top_k": top_k},
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            # The vector-service returns a LIST of results. Also support dict forms.
            if isinstance(result, list):
                return result
            if isinstance(result, dict):
                return result.get("results", result.get("data", []))
            return []
        else:
            logger.warning(f"Vector search failed: {response.status_code}")
            return []
            
    except requests.exceptions.Timeout:
        logger.error("Vector search timeout")
        return []
    except requests.exceptions.ConnectionError:
        logger.error("Vector search connection error")
        return []
    except Exception as e:
        logger.error(f"Vector search error: {e}")
        return []

def format_context_safe(documents: List[Dict]) -> str:
    """Formate contexte de mani√®re s√©curis√©e"""
    if not documents:
        return "Aucun document de r√©f√©rence disponible."
    
    context = "## DOCUMENTS DE R√âF√âRENCE\n\n"
    for i, doc in enumerate(documents[:6], 1):  # Limiter √† 6 docs
        try:
            doc_text = str(doc.get('text', ''))[:500]  # Limiter texte
            score = float(doc.get('score', 0))
            doc_id = str(doc.get('doc_id', 'N/A'))
            context += f"**[R√©f. {i}]** (Score: {score:.3f}):\n{doc_text}...\n\n"
        except Exception as e:
            logger.warning(f"Error formatting document {i}: {e}")
            continue
    
    return context

def create_optimized_prompt(business_type: str, analysis_type: str, query: str, context: str) -> str:
    """Cr√©e prompts optimis√©s pour √©viter les erreurs"""
    
    # Templates courts et efficaces
    prompt_templates = {
        "finance_banque": f"""ANALYSE BANCAIRE STRAT√âGIQUE

MISSION: {query}

CONTEXTE:
{context[:4000]}  

G√âN√àRE UN RAPPORT PROFESSIONNEL STRUCTUR√â (10+ pages):

# RAPPORT STRAT√âGIQUE BANCAIRE

## üéØ SYNTH√àSE EX√âCUTIVE
- Enjeux transformation sectorielle [R√©f. X]
- Recommandations prioritaires avec ROI
- Timeline et investissements

## üìä ANALYSE SECTORIELLE  
- Taille march√© et croissance [R√©f. X]
- Segmentation clients [R√©f. X]
- Performance secteur [R√©f. X]
- Technologies √©mergentes [R√©f. X]

## ‚öîÔ∏è CONCURRENCE
- Leaders vs challengers [R√©f. X]
- Forces/faiblesses [R√©f. X]
- Strat√©gies diff√©renciation [R√©f. X]

## üí° RECOMMANDATIONS
- Plan action 12-18 mois [R√©f. X]
- Business case ROI [R√©f. X]
- Gestion risques [R√©f. X]

## üìà PROJECTIONS
- Scenarios 2025-2030 [R√©f. X]
- KPIs de suivi [R√©f. X]

Minimum 5000 mots. Cite [R√©f. X] pour toute donn√©e factuelle.""",

        "tech_digital": f"""ANALYSE TRANSFORMATION DIGITALE

MISSION: {query}

CONTEXTE:
{context[:4000]}

G√âN√àRE RAPPORT TECHNIQUE STRAT√âGIQUE (10+ pages):

# RAPPORT TRANSFORMATION DIGITALE

## üéØ VISION EX√âCUTIVE
- Enjeux transformation [R√©f. X]
- ROI digital [R√©f. X]
- Roadmap strat√©gique

## üîß √âTAT DES LIEUX TECH
- Maturit√© technologique [R√©f. X]
- Gaps et opportunit√©s [R√©f. X]
- Benchmark secteur [R√©f. X]

## üöÄ INNOVATION
- Technologies cl√©s [R√©f. X]
- Use cases business [R√©f. X]
- Investissements [R√©f. X]

## üìã PLAN D'ACTION
- Phases transformation [R√©f. X]
- Budget et timeline [R√©f. X]
- Organisation et skills [R√©f. X]

Minimum 5000 mots. R√©f√©rencer [R√©f. X] syst√©matiquement.""",

        "retail_commerce": f"""ANALYSE RETAIL STRAT√âGIQUE

MISSION: {query}

CONTEXTE:
{context[:4000]}

G√âN√àRE RAPPORT RETAIL COMPLET (10+ pages):

# RAPPORT STRAT√âGIE RETAIL

## üéØ SYNTH√àSE RETAIL
- Tendances march√© [R√©f. X]
- Transformation omnicanal [R√©f. X]
- Strat√©gies gagnantes

## üõçÔ∏è MARCH√â ET CLIENTS
- √âvolution consommation [R√©f. X]
- Segments clients [R√©f. X]
- Parcours d'achat [R√©f. X]

## üè™ CONCURRENCE
- Players traditionnels vs pure players [R√©f. X]
- Innovations retail [R√©f. X]
- Diff√©renciation [R√©f. X]

## üí° RECOMMANDATIONS
- Strat√©gie omnicanal [R√©f. X]
- Technologies retail [R√©f. X]
- Plan d√©ploiement [R√©f. X]

Minimum 5000 mots. Citer [R√©f. X] pour donn√©es factuelles."""
    }
    
    return prompt_templates.get(business_type, prompt_templates["finance_banque"])

def call_openai_safe(prompt: str, business_type: str) -> str:
    """Appel OpenAI s√©curis√© avec gestion d'erreurs compl√®te"""
    try:
        if not OPENAI_API_KEY or OPENAI_API_KEY == "":
            return "‚ö†Ô∏è **Configuration OpenAI requise**\n\nVeuillez configurer la variable OPENAI_API_KEY dans votre fichier .env"
        
        # V√©rifier OpenAI
        if not OPENAI_AVAILABLE:
            return "‚ùå **Module OpenAI manquant**\n\nVeuillez installer: pip install openai"
        
        # System prompts par m√©tier
        system_prompts = {
            "finance_banque": """Tu es un consultant senior McKinsey sp√©cialis√© en strat√©gie bancaire. 
                              G√©n√®re des rapports structur√©s avec analyses quantifi√©es et recommandations actionnables.
                              Utilise exclusivement les donn√©es des documents fournis avec r√©f√©rences [R√©f. X].""",
                              
            "tech_digital": """Tu es un consultant BCG expert en transformation digitale. 
                             G√©n√®re des analyses techniques d√©taill√©es avec business case et ROI.
                             Base tes analyses sur les documents fournis avec r√©f√©rences [R√©f. X].""",
                             
            "retail_commerce": """Tu es un consultant Bain expert en retail et commerce. 
                                G√©n√®re des analyses avec insights consommateurs et recommandations op√©rationnelles.
                                Utilise les documents fournis avec r√©f√©rences [R√©f. X]."""
        }
        
        system_prompt = system_prompts.get(business_type, system_prompts["finance_banque"])
        
        # Client OpenAI avec param√®tres explicites
        try:
            client = OpenAI(
                api_key=OPENAI_API_KEY,
                timeout=30.0
            )
            
            # V√©rifier taille prompt
            if len(prompt) > 15000:
                logger.warning(f"Prompt tr√®s long ({len(prompt)} chars), troncature appliqu√©e")
                prompt = prompt[:15000] + "\n\n[...Prompt tronqu√© pour limites techniques. Continuer l'analyse avec les √©l√©ments disponibles...]"
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=8000
            )
            
            return response.choices[0].message.content
            
        except Exception as api_error:
            logger.error(f"OpenAI API error: {api_error}")
            return f"‚ùå **Erreur API OpenAI**\n\n{str(api_error)[:300]}\n\nV√©rifiez votre cl√© API et votre quota."
        
    except Exception as e:
        logger.error(f"Critical error in OpenAI call: {e}")
        return f"‚ùå **Erreur critique**\n\n{str(e)[:300]}"

async def generate_business_analysis_safe(business_type: str, analysis_type: str, query: str, title: str = None) -> AnalysisResponse:
    """G√©n√®re analyse avec gestion d'erreurs compl√®te"""
    try:
        logger.info(f"Starting analysis: {business_type}/{analysis_type}")
        
        # 1. Recherche documents s√©curis√©e
        documents = search_documents_safe(query, top_k=8)
        logger.info(f"Found {len(documents)} documents")
        
        # 2. Formatage contexte s√©curis√©
        context = format_context_safe(documents)
        
        # 3. Cr√©ation prompt optimis√©
        prompt = create_optimized_prompt(business_type, analysis_type, query, context)
        
        # 4. Appel OpenAI s√©curis√©
        content = call_openai_safe(prompt, business_type)
        
        # 5. Construction r√©ponse
        return AnalysisResponse(
            analysis_type=analysis_type,
            business_type=business_type,
            title=title or f"Rapport {get_business_type_display_name(business_type)} - {analysis_type.replace('_', ' ').title()}",
            content=content,
            sources=[{
                "doc_id": d.get("doc_id", "N/A"),
                "score": d.get("score", 0),
                "text": str(d.get("text", ""))[:200]
            } for d in documents],
            metadata={
                "query": query,
                "business_type": business_type,
                "documents_found": len(documents),
                "analysis_length": "extended_report",
                "model": "gpt-4o-mini",
                "max_tokens": 8000,
                "status": "success"
            },
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Error in business analysis: {e}")
        # Retourner une r√©ponse d'erreur plut√¥t qu'une exception
        return AnalysisResponse(
            analysis_type=analysis_type,
            business_type=business_type,
            title=title or "Analyse √©chou√©e",
            content=f"‚ùå **Erreur lors de l'analyse**\n\n{str(e)[:500]}\n\nVeuillez r√©essayer ou contacter le support.",
            sources=[],
            metadata={
                "query": query,
                "business_type": business_type,
                "error": str(e),
                "status": "failed"
            },
            timestamp=datetime.now().isoformat()
        )

async def generate_chat_response_safe(message: str, business_type: str = None, history: List[Dict] = None) -> ChatResponse:
    """Chat avec gestion d'erreurs robuste"""
    try:
        # 1. Recherche documents
        documents = search_documents_safe(message, top_k=5)
        context = format_context_safe(documents)
        
        # 2. Construction prompt chat
        business_context = get_business_type_display_name(business_type) if business_type else "G√©n√©raliste"
        
        chat_prompt = f"""Tu es un assistant expert sp√©cialis√© {business_context}.

CONTEXTE DOCUMENTAIRE:
{context}

HISTORIQUE CONVERSATION:
{history[-3:] if history else "Nouvelle conversation"}

QUESTION: {message}

R√©ponds de mani√®re concise et professionnelle en te basant sur les documents fournis.
Si la question d√©passe ton domaine d'expertise, oriente vers le bon sp√©cialiste.
Cite [R√©f. X] pour les informations factuelles.
"""

        # 3. Appel OpenAI s√©curis√©
        response_content = call_openai_safe(chat_prompt, business_type or "finance_banque")
        
        return ChatResponse(
            response=response_content,
            business_context=business_context,
            sources=[{
                "doc_id": d.get("doc_id", "N/A"),
                "score": d.get("score", 0),
                "text": str(d.get("text", ""))[:100]
            } for d in documents[:3]],
            metadata={
                "message": message,
                "business_type": business_type,
                "documents_found": len(documents),
                "model": "gpt-4o-mini"
            },
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Error in chat response: {e}")
        return ChatResponse(
            response=f"‚ùå Erreur dans la r√©ponse: {str(e)[:200]}",
            business_context=business_type or "Error",
            sources=[],
            metadata={"error": str(e)},
            timestamp=datetime.now().isoformat()
        )

# Endpoints
@app.get("/health")
def health():
    """Health check avec diagnostics √©tendus"""
    return {
        "status": "healthy", 
        "service": "backend-intelligence-fixed",
        "openai_configured": bool(OPENAI_API_KEY),
        "vector_service": VECTOR_SERVICE_URL,
        "business_types": get_available_business_types(),
        "version": "1.0-robust"
    }

@app.get("/business-types")
def get_business_types():
    """Types de m√©tier disponibles"""
    try:
        return {
            "business_types": [
                {"key": bt, "display_name": get_business_type_display_name(bt)} 
                for bt in get_available_business_types()
            ]
        }
    except Exception as e:
        logger.error(f"Error getting business types: {e}")
        return {
            "business_types": [
                {"key": "finance_banque", "display_name": "üè¶ Finance & Banque"},
                {"key": "tech_digital", "display_name": "üíª Tech & Digital"},
                {"key": "retail_commerce", "display_name": "üõçÔ∏è Retail & Commerce"}
            ]
        }

@app.post("/extended-analysis", response_model=AnalysisResponse)
async def extended_analysis(request: BusinessAnalysisRequest):
    """G√©n√®re rapports longs style cabinet conseil - Version robuste"""
    return await generate_business_analysis_safe(
        request.business_type,
        request.analysis_type,
        request.query,
        request.title
    )

@app.post("/business-analysis", response_model=AnalysisResponse)
async def business_analysis(request: BusinessAnalysisRequest):
    """Alias pour compatibilit√©"""
    return await generate_business_analysis_safe(
        request.business_type,
        request.analysis_type,
        request.query,
        request.title
    )

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Chat intelligent avec contexte m√©tier"""
    return await generate_chat_response_safe(
        request.message,
        request.business_type,
        request.conversation_history
    )

@app.post("/chat/stream")
async def chat_stream_endpoint(request: ChatRequest):
    """Streaming de la r√©ponse du chat (texte brut chunk√©, compatible fetch streaming)."""
    async def token_generator():
        try:
            # 1) RAG rapide (synchrone)
            documents = search_documents_safe(request.message, top_k=5)
            context = format_context_safe(documents)
            business_context = get_business_type_display_name(request.business_type) if request.business_type else "G√©n√©raliste"
            chat_prompt = f"""Tu es un assistant expert sp√©cialis√© {business_context}.

CONTEXTE DOCUMENTAIRE:
{context}

HISTORIQUE CONVERSATION:
{request.conversation_history[-3:] if request.conversation_history else "Nouvelle conversation"}

QUESTION: {request.message}

R√©ponds de mani√®re concise et professionnelle en te basant sur les documents fournis.
Si la question d√©passe ton domaine d'expertise, oriente vers le bon sp√©cialiste.
Cite [R√©f. X] pour les informations factuelles.
"""

            # 2) Streaming OpenAI
            if not OPENAI_API_KEY or not OPENAI_AVAILABLE:
                # Fallback non‚Äëbloquant
                yield "Le streaming n√©cessite une configuration OPENAI_API_KEY.\n"
                yield "[DONE]"
                return

            client = OpenAI(api_key=OPENAI_API_KEY, timeout=30.0)
            stream = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"Assistant sp√©cialis√© {business_context}."},
                    {"role": "user", "content": chat_prompt}
                ],
                temperature=0.3,
                max_tokens=1500,
                stream=True,
            )

            for event in stream:
                try:
                    delta = event.choices[0].delta if hasattr(event.choices[0], "delta") else event.choices[0].get("delta", {})
                    content = getattr(delta, "content", None)
                    if content is None and isinstance(delta, dict):
                        content = delta.get("content")
                    if content:
                        yield content
                except Exception as inner:
                    logger.warning(f"Stream delta parse error: {inner}")
                    continue

            yield "[DONE]"
        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield "\n[STREAM_ERROR]"

    headers = {
        "Cache-Control": "no-cache",
        "X-Accel-Buffering": "no",  # Nginx: disable buffering
    }
    return StreamingResponse(token_generator(), media_type="text/plain", headers=headers)

@app.get("/test-openai")
async def test_openai():
    """Test de connectivit√© OpenAI"""
    try:
        if not OPENAI_API_KEY:
            return {"status": "error", "message": "OPENAI_API_KEY not configured"}
        
        client = OpenAI(
            api_key=OPENAI_API_KEY,
            timeout=30.0
        )
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "Hello, test simple"}],
            max_tokens=10
        )
        
        return {
            "status": "success", 
            "message": "OpenAI API functional",
            "model": "gpt-4o-mini",
            "response": response.choices[0].message.content
        }
        
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/diagnostics")
async def diagnostics():
    """Diagnostics complets du syst√®me"""
    
    diagnostics_result = {
        "timestamp": datetime.now().isoformat(),
        "service": "backend-intelligence-fixed",
        "version": "1.0-robust"
    }
    
    # Versions des libs cl√©s
    try:
        diagnostics_result["versions"] = {
            "python": os.getenv("PYTHON_VERSION", "unknown"),
            "openai": metadata.version("openai") if OPENAI_AVAILABLE else "not-installed",
            "httpx": metadata.version("httpx") if "httpx" in {d.metadata["Name"].lower() for d in map(lambda n: metadata.distribution(n), metadata.packages_distributions().keys()) if False} else metadata.version("httpx")
        }
    except Exception:
        try:
            diagnostics_result["versions"] = {
                "openai": metadata.version("openai") if OPENAI_AVAILABLE else "not-installed",
                "httpx": metadata.version("httpx")
            }
        except Exception as e:
            diagnostics_result["versions_error"] = str(e)

    # Proxies d'environnement (pour debug)
    diagnostics_result["env_proxies"] = {
        k: os.getenv(k)
        for k in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"]
        if os.getenv(k)
    }

    # Test OpenAI
    try:
        if OPENAI_API_KEY:
            client = OpenAI(
                api_key=OPENAI_API_KEY,
                timeout=30.0
            )
            test_response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            diagnostics_result["openai"] = {"status": "‚úÖ Functional", "model": "gpt-4o-mini"}
        else:
            diagnostics_result["openai"] = {"status": "‚ùå Not configured", "model": None}
    except Exception as e:
        diagnostics_result["openai"] = {"status": f"‚ùå Error: {str(e)[:100]}", "model": "gpt-4o-mini"}
    
    # Test Vector Service
    try:
        test_docs = search_documents_safe("test", top_k=1)
        diagnostics_result["vector_service"] = {
            "status": "‚úÖ Accessible" if len(test_docs) >= 0 else "‚ö†Ô∏è No results",
            "url": VECTOR_SERVICE_URL,
            "test_results": len(test_docs)
        }
    except Exception as e:
        diagnostics_result["vector_service"] = {"status": f"‚ùå Error: {str(e)[:100]}", "url": VECTOR_SERVICE_URL}
    
    # Test Business Types
    try:
        business_types = get_available_business_types()
        diagnostics_result["business_types"] = {"status": "‚úÖ Available", "count": len(business_types), "types": business_types}
    except Exception as e:
        diagnostics_result["business_types"] = {"status": f"‚ùå Error: {str(e)[:100]}"}
    
    return diagnostics_result

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8006)