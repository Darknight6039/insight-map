# üéØ Configuration des mod√®les Sonar (Optimisation v3.1)

## Vue d'ensemble

L'application utilise une strat√©gie multi-mod√®les Perplexity optimis√©e par cas d'usage pour maximiser la qualit√© des r√©ponses tout en minimisant les co√ªts d'API. Chaque endpoint s√©lectionne automatiquement le mod√®le le plus adapt√© √† sa t√¢che.

## Strat√©gie multi-mod√®les

### Tableau comparatif

| Mod√®le | Usage | Max Tokens | Cost/1K tokens | Endpoints |
|--------|-------|------------|----------------|-----------|
| `sonar` | Chat rapide, tests | 4000 | $0.001 | `/chat`, `/chat/stream`, `/test-perplexity` |
| `sonar-pro` | Rapports longs (5000+ mots) | 8000 | $0.003 | `/extended-analysis`, `/business-analysis` |
| `sonar-reasoning` | Analyses complexes | 8000 | $0.005 | Configurable pour analyses expertes |

### Principes de s√©lection

**1. Chat conversationnel** ‚Üí `sonar`
- Questions courtes et r√©ponses rapides
- Tests API et validation
- Co√ªt optimis√© (3x moins cher que sonar-pro)
- Suffisant pour 95% des conversations

**2. Rapports strat√©giques** ‚Üí `sonar-pro`
- G√©n√©ration de rapports longs (5000-8000 mots)
- Analyses sectorielles approfondies
- Web search profond avec citations multiples
- Qualit√© maximale pour livrables clients

**3. Analyses expertes** ‚Üí `sonar-reasoning`
- Raisonnement structur√© multi-√©tapes
- Analyses de risques complexes
- Mod√©lisation de sc√©narios
- Qualit√© maximale pour cas critiques

## Configuration

### Variables d'environnement

Copier ces lignes dans votre fichier `.env` :

```bash
# Perplexity API Key (obligatoire)
PERPLEXITY_API_KEY=pplx-xxxxxxxxxxxxx

# Configuration multi-mod√®les (optionnel, valeurs par d√©faut ci-dessous)
PERPLEXITY_MODEL_CHAT=sonar
PERPLEXITY_MODEL_ANALYSIS=sonar-pro
PERPLEXITY_MODEL_REASONING=sonar-reasoning
```

### Valeurs par d√©faut

Si les variables `PERPLEXITY_MODEL_*` ne sont pas d√©finies, le syst√®me utilise automatiquement :
- Chat : `sonar`
- Analysis : `sonar-pro`
- Reasoning : `sonar-reasoning`

**Backward compatibility** : L'ancienne variable `PERPLEXITY_MODEL` n'est plus utilis√©e mais peut rester pr√©sente sans effet.

## Optimisation des co√ªts

### R√©duction de co√ªts estim√©e

**Avant optimisation** (v3.0) :
- Tous les endpoints ‚Üí `sonar` (4000 tokens)
- Co√ªt uniforme mais rapports tronqu√©s

**Apr√®s optimisation** (v3.1) :
- Chat endpoints ‚Üí `sonar` : **-70% de co√ªt**
- Analysis endpoints ‚Üí `sonar-pro` : qualit√© maximale
- **√âconomie globale estim√©e : ~60%** sur volume d'API typique (80% chat / 20% analysis)

### Exemple de calcul

Pour 100 000 requ√™tes mensuelles (80% chat / 20% analysis) :

**Avant** (tout en sonar-pro) :
- 100 000 requ√™tes √ó 1500 tokens moy. √ó $0.003 = **$450/mois**

**Apr√®s** (mix optimis√©) :
- 80 000 chat √ó 800 tokens √ó $0.001 = $64
- 20 000 analysis √ó 3000 tokens √ó $0.003 = $180
- **Total : $244/mois** ‚Üí **√âconomie de $206/mois (-46%)**

## S√©lection automatique

### Mapping endpoint ‚Üí mod√®le

Le backend s√©lectionne automatiquement le mod√®le selon l'endpoint appel√© :

```python
# Endpoints chat (sonar)
POST /chat                 ‚Üí sonar (4000 tokens)
POST /chat/stream          ‚Üí sonar (1500 tokens streaming)

# Endpoints analysis (sonar-pro)
POST /extended-analysis    ‚Üí sonar-pro (8000 tokens)
POST /business-analysis    ‚Üí sonar-pro (8000 tokens)

# Endpoints test (tous mod√®les)
GET /test-perplexity       ‚Üí Teste les 3 mod√®les
```

### V√©rification dans les logs

Le syst√®me log automatiquement le mod√®le s√©lectionn√© pour chaque requ√™te :

```bash
# Voir les s√©lections de mod√®les en temps r√©el
docker-compose logs -f backend-service | grep "Using model"
```

Exemple de logs attendus :
```
INFO: Using model: sonar-pro for task: analysis (max_tokens: 8000)
INFO: Using model: sonar for task: chat (max_tokens: 4000)
INFO: Using model: sonar for task: chat (max_tokens: 4000)
INFO: Using model: sonar-pro for task: analysis (max_tokens: 8000)
```

## Validation et tests

### 1. Health check

V√©rifier que la configuration multi-mod√®les est bien active :

```bash
curl http://localhost:8006/health | jq
```

R√©ponse attendue :
```json
{
  "status": "healthy",
  "service": "backend-intelligence-perplexity",
  "perplexity_configured": true,
  "perplexity_models": {
    "chat": "sonar",
    "analysis": "sonar-pro",
    "reasoning": "sonar-reasoning"
  },
  "version": "3.1-multi-model"
}
```

### 2. Test multi-mod√®les

Tester la connectivit√© pour chaque mod√®le configur√© :

```bash
curl http://localhost:8006/test-perplexity | jq
```

R√©ponse attendue :
```json
{
  "status": "success",
  "models_tested": {
    "chat": {
      "model": "sonar",
      "status": "‚úÖ OK",
      "response": "Hello! How can I assist you today?"
    },
    "analysis": {
      "model": "sonar-pro",
      "status": "‚úÖ OK",
      "response": "I'm ready to help with detailed analysis."
    },
    "reasoning": {
      "model": "sonar-reasoning",
      "status": "‚úÖ OK",
      "response": "Let me reason through this systematically."
    }
  },
  "config": {
    "chat": "sonar",
    "analysis": "sonar-pro",
    "reasoning": "sonar-reasoning"
  }
}
```

### 3. Test rapport long

V√©rifier que les rapports utilisent bien `sonar-pro` :

```bash
curl -X POST http://localhost:8006/extended-analysis \
  -H "Content-Type: application/json" \
  -d '{
    "business_type": "finance_banque",
    "analysis_type": "synthese_executive",
    "query": "Analyse du march√© bancaire fran√ßais 2025"
  }' | jq '.metadata.model'
```

R√©sultat attendu : `"sonar-pro"`

### 4. Test chat court

V√©rifier que le chat utilise bien `sonar` :

```bash
curl -X POST http://localhost:8006/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Quelles sont les tendances fintech ?",
    "business_type": "finance_banque"
  }' | jq '.metadata.model'
```

R√©sultat attendu : `"sonar"`

## Monitoring et observabilit√©

### Commandes de monitoring

**1. Suivre les s√©lections de mod√®les en temps r√©el**
```bash
docker-compose logs -f backend-service | grep -E "Using model|API error"
```

**2. Compter les requ√™tes par mod√®le (derni√®re heure)**
```bash
docker-compose logs --since 1h backend-service | grep "Using model" | sort | uniq -c
```

Exemple de sortie :
```
     142 INFO: Using model: sonar for task: chat (max_tokens: 4000)
      18 INFO: Using model: sonar-pro for task: analysis (max_tokens: 8000)
       3 INFO: Using model: sonar-reasoning for task: reasoning (max_tokens: 8000)
```

**3. D√©tecter les erreurs par mod√®le**
```bash
docker-compose logs --since 1h backend-service | grep "API error with"
```

### Patterns de logs

**‚úÖ Patterns normaux :**
```
INFO: Using model: sonar-pro for task: analysis (max_tokens: 8000)
INFO: Using model: sonar for task: chat (max_tokens: 4000)
```

**‚ùå Patterns d'erreur √† surveiller :**
```
ERROR: Perplexity API error with sonar-pro: 401 Unauthorized
ERROR: Perplexity API error with sonar: 429 Rate limit exceeded
ERROR: Perplexity API error with sonar-reasoning: 404 Model not found
```

## Troubleshooting

### Probl√®me : Erreur 404 Model not found

**Cause** : Le mod√®le configur√© n'existe pas ou n'est pas accessible avec votre cl√© API.

**Solution** :
1. V√©rifier les mod√®les disponibles sur votre compte Perplexity
2. Mettre √† jour `.env` avec les noms de mod√®les corrects
3. Red√©marrer le service : `docker-compose restart backend-service`

### Probl√®me : Tous les endpoints utilisent le m√™me mod√®le

**Cause** : Variables d'environnement non d√©finies ou non charg√©es.

**Solution** :
1. V√©rifier `.env` contient bien les 3 variables `PERPLEXITY_MODEL_*`
2. Rebuild et red√©marrer : `docker-compose up -d --build backend-service`
3. V√©rifier `/health` affiche bien `"perplexity_models": {...}`

### Probl√®me : Erreur 429 Rate limit

**Cause** : Quota API d√©pass√©.

**Solution** :
1. V√©rifier votre quota sur https://www.perplexity.ai/settings/api
2. R√©duire temporairement le trafic ou augmenter votre plan
3. Impl√©menter un rate limiting c√¥t√© application si n√©cessaire

### Probl√®me : Rapports tronqu√©s malgr√© sonar-pro

**Cause** : Prompt trop long d√©passant les limites du mod√®le.

**V√©rification** :
```bash
docker-compose logs backend-service | grep "Prompt tr√®s long"
```

**Solution** : Le syst√®me tronque automatiquement √† 15 000 caract√®res. Si n√©cessaire, r√©duire le nombre de documents RAG dans la requ√™te.

## Impact business

### Qualit√© des livrables

**Chat** (sonar) :
- ‚úÖ R√©ponses rapides et pertinentes pour questions courtes
- ‚úÖ Citations web avec sources v√©rifiables
- ‚úÖ Latence r√©duite (<2s en moyenne)

**Rapports** (sonar-pro) :
- ‚úÖ G√©n√©ration de 5000-8000 mots structur√©s
- ‚úÖ Recherche web approfondie avec 10+ sources
- ‚úÖ Format cabinet de conseil professionnel
- ‚úÖ Citations APA acad√©miques compl√®tes

**Analyses expertes** (sonar-reasoning) :
- ‚úÖ Raisonnement structur√© multi-√©tapes
- ‚úÖ Mod√©lisation de sc√©narios complexes
- ‚úÖ Qualit√© maximale pour cas critiques

### ROI de l'optimisation

**Pour une utilisation typique** (startup/PME) :
- **Avant** : ~$450/mois (tout en sonar-pro)
- **Apr√®s** : ~$244/mois (mix optimis√©)
- **√âconomie** : $206/mois soit **$2 472/an**

**Pour une utilisation intensive** (cabinet conseil) :
- **Avant** : ~$2 000/mois
- **Apr√®s** : ~$1 080/mois
- **√âconomie** : $920/mois soit **$11 040/an**

## D√©ploiement en production

### Checklist de d√©ploiement

- [ ] Copier `.env.example` vers `.env` avec vraie cl√© API
- [ ] D√©finir les 3 variables `PERPLEXITY_MODEL_*`
- [ ] Build du service : `docker-compose build backend-service`
- [ ] Red√©marrage : `docker-compose up -d backend-service`
- [ ] Validation `/health` affiche version `3.1-multi-model`
- [ ] Test `/test-perplexity` affiche ‚úÖ OK pour les 3 mod√®les
- [ ] Test rapport long v√©rifie utilisation `sonar-pro`
- [ ] Test chat v√©rifie utilisation `sonar`
- [ ] Monitoring logs actif pour d√©tection d'erreurs

### Strat√©gie de rollback

Si probl√®me en production, revenir √† la version pr√©c√©dente :

```bash
# 1. Downgrade du code
git checkout v3.0

# 2. Rebuild
docker-compose build backend-service

# 3. Red√©marrage
docker-compose up -d backend-service

# 4. V√©rification
curl http://localhost:8006/health
```

## √âvolutions futures

### V3.2 - S√©lection dynamique avanc√©e

- **D√©tection automatique** de la longueur de r√©ponse attendue
- **Switch intelligent** chat court (sonar) vs chat long (sonar-pro)
- **Fallback automatique** si mod√®le indisponible

### V3.3 - Optimisation par utilisateur

- **Profiling utilisateur** : pr√©f√©rences qualit√©/co√ªt par compte
- **Budget mensuel** : basculement auto vers mod√®les √©conomiques
- **Analytics co√ªts** : dashboard temps r√©el de consommation

### V4.0 - Support multi-providers

- **Perplexity** : Sonar (default)
- **OpenAI** : GPT-4o (fallback)
- **Anthropic** : Claude 3.5 Sonnet (option qualit√©)
- **Local** : Ollama/LLaMA (option privacy)

## Support et ressources

### Documentation officielle

- [Perplexity API Docs](https://docs.perplexity.ai/)
- [Sonar Models Reference](https://docs.perplexity.ai/docs/model-cards)
- [Pricing Calculator](https://www.perplexity.ai/settings/api)

### Contacts internes

- **Tech Lead** : support-technique@example.com
- **Product** : product@example.com
- **Slack** : #perplexity-integration

### Changelog

**v3.1 (2025-01-15)** :
- ‚úÖ Impl√©mentation strat√©gie multi-mod√®les
- ‚úÖ S√©lection automatique par endpoint
- ‚úÖ Monitoring et observabilit√©
- ‚úÖ Documentation compl√®te

**v3.0 (2025-01-01)** :
- ‚úÖ Migration vers Perplexity API
- ‚úÖ Web search natif avec citations
- ‚úÖ D√©sactivation RAG interne

**v2.0 (2024-12-01)** :
- ‚úÖ Architecture microservices
- ‚úÖ RAG avec Qdrant
- ‚úÖ Rapports PDF professionnels

---

**üéØ Configuration v3.1 - Multi-Model Sonar Strategy**  
*Optimisez vos co√ªts tout en maximisant la qualit√© de vos analyses IA*

