"""
RAG Service Final - Version qui fonctionne avec vraie recherche vectorielle et format professionnel
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
import os
import requests
from datetime import datetime
from loguru import logger

app = FastAPI(title="RAG Service Final", description="RAG avec recherche vectorielle")

# Configuration CORS pour permettre les requ√™tes depuis le frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifier les origines autoris√©es
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
VECTOR_SERVICE_URL = "http://vector-service:8002"

# Mod√®les
class AnalysisRequest(BaseModel):
    query: str
    title: Optional[str] = None

class AnalysisResponse(BaseModel):
    analysis_type: str
    title: str
    content: str
    sources: List[Dict]
    metadata: Dict
    timestamp: str

def search_documents(query: str, top_k: int = 8) -> List[Dict]:
    """Recherche vectorielle avec fallback"""
    try:
        response = requests.post(
            f"{VECTOR_SERVICE_URL}/search",
            json={"query": query, "top_k": top_k},
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            # Vector-service returns a list of results; support both list and dict
            if isinstance(result, list):
                return result
            if isinstance(result, dict):
                return result.get("results", result.get("data", []))
            return []
        else:
            logger.warning(f"Vector search failed: {response.status_code}")
            # Retourner une r√©ponse simul√©e pour continuer le service
            return []
            
    except Exception as e:
        logger.error(f"Vector search error: {e}")
        return []

def create_rag_prompt(analysis_type: str, query: str, documents: List[Dict]) -> str:
    """Cr√©e un prompt avec contexte documentaire"""
    
    # Contexte des documents
    context = ""
    if documents:
        context = "## DOCUMENTS DE R√âF√âRENCE\n\n"
        for i, doc in enumerate(documents[:5], 1):
            doc_text = doc.get('text', '')[:300]
            context += f"**Document {i}** (Score: {doc.get('score', 0):.3f}):\n{doc_text}...\n\n"
    
    # Templates structur√©s
    templates = {
        "synthese_executive": f"""
Bas√© sur les documents de r√©f√©rence, g√©n√®re une SYNTH√àSE EX√âCUTIVE structur√©e.

{context}

DEMANDE: {query}

FORMAT OBLIGATOIRE:

# SYNTH√àSE EX√âCUTIVE

## üéØ R√âSUM√â STRAT√âGIQUE
[Points cl√©s avec donn√©es chiffr√©es des documents]

## üí° INSIGHTS MAJEURS  
[D√©couvertes importantes des sources]

## ‚ö° RECOMMANDATIONS PRIORITAIRES
### Actions imm√©diates (0-3 mois)
[Actions concr√®tes avec justifications]

### Initiatives moyen terme (3-12 mois)
[Initiatives strat√©giques]

## üìä M√âTRIQUES CL√âS
[Indicateurs avec benchmarks]

## ‚ö†Ô∏è RISQUES √Ä SURVEILLER
[Points d'attention majeurs]

IMPORTANT: Utilise UNIQUEMENT les informations des documents fournis. Cite [R√©f. X] pour chaque donn√©e.
        """,
        
        "analyse_concurrentielle": f"""
Analyse concurrentielle bas√©e sur les documents de r√©f√©rence.

{context}

DEMANDE: {query}

FORMAT OBLIGATOIRE:

# ANALYSE CONCURRENTIELLE

## üó∫Ô∏è CARTOGRAPHIE DU MARCH√â
[Acteurs et parts de march√© des documents]

## ‚öîÔ∏è FORCES/FAIBLESSES
### Leaders
[Analyse des leaders]

### Challengers  
[Positionnement challengers]

## üìà DYNAMIQUES MARCH√â
[Tendances concurrentielles]

## üéØ OPPORTUNIT√âS
[Gaps et recommandations]

Cite syst√©matiquement [R√©f. X] pour chaque information.
        """,
        
        "veille_technologique": f"""
Veille technologique bas√©e sur les documents.

{context}

DEMANDE: {query}

FORMAT OBLIGATOIRE:

# VEILLE TECHNOLOGIQUE

## üî¨ TECHNOLOGIES √âMERGENTES
[Innovations des documents]

## üöÄ TENDANCES DISRUPTIVES
[Technologies transformatrices]

## üíº APPLICATIONS SECTORIELLES
[Cas d'usage concrets]

## üîÆ PROJECTIONS
[Timeline et adoption]

R√©f√©rence chaque information [R√©f. X].
        """,
        
        "analyse_risques": f"""
Analyse des risques bas√©e sur les documents.

{context}

DEMANDE: {query}

FORMAT OBLIGATOIRE:

# ANALYSE DES RISQUES

## üö® CARTOGRAPHIE RISQUES
### Risques Strat√©giques
[Menaces business]

### Risques Op√©rationnels
[Risques process]

### Risques R√©glementaires
[Compliance]

## üìä √âVALUATION IMPACT
[Matrice probabilit√©/impact]

## üõ°Ô∏è MITIGATION
[Mesures pr√©ventives et contingence]

Cite [R√©f. X] pour chaque risque identifi√©.
        """,
        
        "etude_marche": f"""
√âtude de march√© bas√©e sur les documents.

{context}

DEMANDE: {query}

FORMAT OBLIGATOIRE:

# √âTUDE DE MARCH√â

## üìè TAILLE DU MARCH√â
[Dimensionnement avec chiffres]

## üë• ANALYSE DEMANDE
[Comportements clients]

## üè¢ STRUCTURE OFFRE
[Acteurs et parts]

## üí∞ DYNAMIQUES √âCONOMIQUES
[Pricing et rentabilit√©]

## üîÆ PROJECTIONS
[√âvolution 3-5 ans]

## üéØ OPPORTUNIT√âS
[Recommandations d'investissement]

R√©f√©rence [R√©f. X] pour chaque donn√©e march√©.
        """
    }
    
    return templates.get(analysis_type, templates["synthese_executive"])

def call_openai(prompt: str) -> str:
    """Appel OpenAI avec gestion d'erreur"""
    try:
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your_openai_api_key_here":
            return "‚ö†Ô∏è Configuration OpenAI requise pour analyses avec vos documents."
        
        from openai import OpenAI
        client = OpenAI(api_key=OPENAI_API_KEY)
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Tu es un consultant expert qui g√©n√®re des rapports professionnels bas√©s UNIQUEMENT sur les documents fournis."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=4000
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        logger.error(f"OpenAI error: {e}")
        return f"Erreur d'analyse: {str(e)}"

def format_sources(documents: List[Dict]) -> str:
    """Formate les sources"""
    if not documents:
        return ""
    
    sources = "\n\n## üìö SOURCES ET R√âF√âRENCES\n\n"
    for i, doc in enumerate(documents[:5], 1):
        text_preview = doc.get('text', '')[:100]
        score = doc.get('score', 0)
        sources += f"**[R√©f. {i}]** Score: {score:.3f} - \"{text_preview}...\"\n\n"
    
    return sources

async def generate_analysis(analysis_type: str, query: str, title: str = None) -> AnalysisResponse:
    """G√©n√®re analyse avec RAG"""
    try:
        # 1. Recherche vectorielle
        logger.info(f"Recherche pour: {query}")
        documents = search_documents(query, top_k=8)
        
        # 2. Cr√©ation du prompt RAG
        prompt = create_rag_prompt(analysis_type, query, documents)
        
        # 3. Appel OpenAI
        content = call_openai(prompt)
        
        # 4. Ajout des sources
        if documents and content:
            content += format_sources(documents)
        
        return AnalysisResponse(
            analysis_type=analysis_type,
            title=title or f"Analyse {analysis_type.replace('_', ' ').title()}",
            content=content,
            sources=[{
                "doc_id": d.get("doc_id"),
                "score": d.get("score"),
                "text": d.get("text", "")[:200]
            } for d in documents],
            metadata={
                "query": query,
                "documents_found": len(documents),
                "vector_search": "active" if documents else "no_results",
                "model": "gpt-4o-mini"
            },
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Error in analysis: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Endpoints
@app.get("/health")
def health():
    return {"status": "healthy", "service": "rag-final", "model": "gpt-4o-mini"}

@app.post("/synthesize", response_model=AnalysisResponse)
async def synthesize(request: AnalysisRequest):
    """Synth√®se ex√©cutive avec RAG"""
    return await generate_analysis("synthese_executive", request.query, request.title)

@app.post("/analyze_competition", response_model=AnalysisResponse)
async def analyze_competition(request: AnalysisRequest):
    """Analyse concurrentielle avec RAG"""
    return await generate_analysis("analyse_concurrentielle", request.query, request.title)

@app.post("/tech_watch", response_model=AnalysisResponse)
async def tech_watch(request: AnalysisRequest):
    """Veille technologique avec RAG"""
    return await generate_analysis("veille_technologique", request.query, request.title)

@app.post("/risk_analysis", response_model=AnalysisResponse)
async def risk_analysis(request: AnalysisRequest):
    """Analyse des risques avec RAG"""
    return await generate_analysis("analyse_risques", request.query, request.title)

@app.post("/market_study", response_model=AnalysisResponse)
async def market_study(request: AnalysisRequest):
    """√âtude de march√© avec RAG"""
    return await generate_analysis("etude_marche", request.query, request.title)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
