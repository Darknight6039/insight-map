# Configuration Mod√®les Perplexity

**Date**: 15 novembre 2025  
**Version**: v4.0-sonar-pro-exclusive  
**Status**: ‚úÖ Configuration Active

---

## Vue d'Ensemble

L'application utilise une strat√©gie multi-mod√®les Perplexity optimis√©e selon le type de requ√™te, permettant d'optimiser les co√ªts tout en maximisant la qualit√© des rapports.

---

## Configuration Actuelle

### Rapports (Tous Types)

**Mod√®le**: `sonar-pro`  
**Max tokens**: 12 000  
**Timeout**: 7.5 minutes (450s)  
**Temp√©rature**: 0.1  

**Utilisation**:
- ‚úÖ Rapports standards (15-25 sources)
- ‚úÖ Rapports approfondis (60 sources, 8000-10000 mots)
- ‚úÖ Analyses sectorielles
- ‚úÖ √âtudes concurrentielles
- ‚úÖ Veilles technologiques
- ‚úÖ Analyses de risques
- ‚úÖ √âtudes de march√©

**Configuration**:
```python
# backend-service/app/main.py (ligne 47)
"analysis": os.getenv("PERPLEXITY_MODEL_ANALYSIS", "sonar-pro")

# Endpoint impact√©
task_type="analysis"  # Tous les rapports
```

**Caract√©ristiques**:
- Recherche web extensive (native Perplexity)
- Citations multiples et croisement de sources
- G√©n√©ration longue (8000-10000 mots pour rapports approfondis)
- Hi√©rarchie stricte des sources (60% institutionnelles, 20% acad√©miques, 15% m√©dia, 5% autres)

---

### Chat

**Mod√®le**: `sonar`  
**Max tokens**: 6 000  
**Timeout**: 5 minutes (300s)  
**Temp√©rature**: 0.1  

**Utilisation**:
- ‚úÖ Conversations courtes
- ‚úÖ R√©ponses rapides (2-4 paragraphes)
- ‚úÖ 5-8 sources minimum

**Configuration**:
```python
# backend-service/app/main.py (ligne 46)
"chat": os.getenv("PERPLEXITY_MODEL_CHAT", "sonar")

# Endpoints impact√©s
task_type="chat"  # Conversations
```

**Caract√©ristiques**:
- Co√ªt optimis√© (~70% moins cher que sonar-pro)
- R√©ponses concises et sourc√©es
- Recherche web Perplexity pour informations actuelles

---

### Reasoning (Non Utilis√©)

**Mod√®le**: `sonar-reasoning`  
**Max tokens**: 16 000 (configur√© mais non utilis√©)  
**Statut**: R√©serv√© pour usage futur  

**Configuration**:
```python
# backend-service/app/main.py (ligne 48)
"reasoning": os.getenv("PERPLEXITY_MODEL_REASONING", "sonar-reasoning")

# Aucun endpoint ne l'utilise actuellement
```

**Note**: Ce mod√®le est configur√© pour des analyses complexes multi-√©tapes futures mais n'est pas actuellement utilis√© dans l'application.

---

## D√©tails Techniques

### S√©lection Dynamique

Le mod√®le est s√©lectionn√© automatiquement selon le `task_type` pass√© √† la fonction `call_perplexity_safe()`:

```python
def get_model_for_task(task_type: str) -> str:
    """S√©lectionne le mod√®le Sonar appropri√© selon la t√¢che"""
    return PERPLEXITY_MODELS.get(task_type, PERPLEXITY_MODELS["chat"])
```

### Configuration Max Tokens

```python
max_tokens_config = {
    "sonar": 6000,
    "sonar-pro": 12000,  # Rapports standards ET approfondis
    "sonar-reasoning": 16000  # Non utilis√© actuellement
}
```

### Endpoints par Type

| Endpoint | Task Type | Mod√®le | Max Tokens | Usage |
|----------|-----------|--------|------------|-------|
| `/extended-analysis` | `analysis` | `sonar-pro` | 12000 | Rapports d√©taill√©s |
| `/business-analysis` | `analysis` | `sonar-pro` | 12000 | Analyses m√©tier |
| `/chat` | `chat` | `sonar` | 6000 | Conversations |
| `/chat/stream` | `chat` | `sonar` | 6000 | Chat streaming |
| `/test-perplexity` | `chat` | Tous | Varie | Tests config |

---

## Rapports Approfondis (60 Sources)

### D√©tection Automatique

Les rapports approfondis sont d√©tect√©s par la pr√©sence du mot "approfondi" dans `analysis_type`:

```python
if "approfondi" in analysis_type.lower():
    # Template sp√©cial 60 sources
```

### Configuration Sp√©cifique

**Mod√®le**: `sonar-pro` (identique aux rapports standards)  
**Max tokens**: 12 000 (identique, suffisant pour 60 sources)  
**Timeout**: 7.5 minutes (permettant g√©n√©ration longue)  

**Exigences**:
- Minimum 60 sources organis√©es par cat√©gorie
- 36 sources institutionnelles (60%)
- 12 sources acad√©miques (20%)
- 9 sources m√©dia r√©put√© (15%)
- 3 sources compl√©mentaires (5%)
- 50+ donn√©es chiffr√©es avec sources crois√©es
- 5+ tableaux comparatifs d√©taill√©s
- 8000-10000 mots

**Template Prompt**:
```python
prompt_templates_deep = {
    "finance_banque": """
    **FORMAT** : Rapport ultra-d√©taill√© (8000-10000 mots) avec 60 sources MINIMUM
    
    ## HI√âRARCHIE SOURCES STRICTE (60 sources) :
    - 36 sources institutionnelles (60%)
    - 12 sources acad√©miques (20%)
    - 9 sources m√©dia r√©put√© (15%)
    - 3 sources compl√©mentaires (5%)
    """
}
```

---

## Variables d'Environnement

### Fichier `.env`

```bash
# Perplexity API
PERPLEXITY_API_KEY=pplx-xxxxx

# Configuration multi-mod√®les
PERPLEXITY_MODEL_CHAT=sonar
PERPLEXITY_MODEL_ANALYSIS=sonar-pro
PERPLEXITY_MODEL_REASONING=sonar-reasoning
```

### Valeurs par D√©faut

Si les variables ne sont pas d√©finies, l'application utilise les valeurs par d√©faut:
- Chat: `sonar`
- Analysis: `sonar-pro`
- Reasoning: `sonar-reasoning`

---

## Optimisation Co√ªts

### R√©partition Actuelle

| Type Requ√™te | Volume Estim√© | Mod√®le | Co√ªt/1K tokens | Impact |
|--------------|---------------|--------|----------------|--------|
| Chat | 70% | sonar | $0.001 | Optimis√© ‚úÖ |
| Rapports standards | 25% | sonar-pro | $0.003 | Qualit√© max ‚úÖ |
| Rapports approfondis | 5% | sonar-pro | $0.003 | Qualit√© max ‚úÖ |

### √âconomies

- **Chat**: ~70% moins cher que si on utilisait sonar-pro
- **Rapports**: Qualit√© maximale justifiant le co√ªt
- **√âconomie globale estim√©e**: ~50-60% sur co√ªts API totaux

---

## Monitoring

### V√©rifier Configuration

```bash
# Health check avec configuration mod√®les
curl http://localhost:8006/health | jq '.perplexity_models'
```

**Sortie attendue**:
```json
{
  "chat": "sonar",
  "analysis": "sonar-pro",
  "reasoning": "sonar-reasoning"
}
```

### Logs Backend

```bash
# Voir mod√®le utilis√© par requ√™te
docker compose logs -f backend-service | grep "Using model"
```

**Exemples**:
```
INFO: Using model: sonar-pro for task: analysis (max_tokens: 12000)
INFO: Using model: sonar for task: chat (max_tokens: 6000)
```

### Logs Progression Rapports

```bash
# Suivre progression g√©n√©ration rapport
docker compose logs -f backend-service | grep -E '\[.*\]'
```

**Sortie typique**:
```
üìä [1/5] Recherche documents RAG...
‚úì [1/5] Trouv√© 12 documents RAG
üìù [2/5] Formatage contexte documentaire...
‚úì [2/5] Contexte format√© (4823 caract√®res)
üéØ [3/5] Cr√©ation prompt optimis√©...
‚úì [3/5] Prompt cr√©√© (type: 60 sources)
üåê [4/5] Appel Perplexity API (60 sources, estimation: 90-120s)...
INFO: Using model: sonar-pro for task: analysis (max_tokens: 12000)
‚úì [4/5] Contenu g√©n√©r√© par Perplexity
‚úÖ [5/5] Finalisation du rapport...
‚úì [5/5] Rapport finalis√© avec 12 sources RAG
```

---

## Tests

### Test Configuration Mod√®les

```bash
curl http://localhost:8006/test-perplexity | jq
```

**R√©sultat attendu**:
```json
{
  "status": "success",
  "models_tested": {
    "chat": {
      "model": "sonar",
      "status": "‚úÖ OK"
    },
    "analysis": {
      "model": "sonar-pro",
      "status": "‚úÖ OK"
    },
    "reasoning": {
      "model": "sonar-reasoning",
      "status": "‚úÖ OK"
    }
  }
}
```

### Test Rapport Standard

```bash
curl -X POST http://localhost:8006/extended-analysis \
  -H "Content-Type: application/json" \
  -d '{
    "business_type": "finance_banque",
    "analysis_type": "synthese_executive",
    "query": "Analyse march√© bancaire fran√ßais 2025"
  }' | jq '.metadata.model'
```

**R√©sultat attendu**: `"sonar-pro"`

### Test Rapport Approfondi

```bash
curl -X POST http://localhost:8006/extended-analysis \
  -H "Content-Type: application/json" \
  -d '{
    "business_type": "finance_banque",
    "analysis_type": "analyse_approfondie",
    "query": "Analyse exhaustive march√© bancaire fran√ßais"
  }' | jq '.metadata.model'
```

**R√©sultat attendu**: `"sonar-pro"` (identique, mais avec 60 sources)

---

## FAQ

### Pourquoi sonar-pro pour tous les rapports?

**R√©ponse**: sonar-pro offre la meilleure qualit√© pour la g√©n√©ration longue (8000-10000 mots) avec recherche web extensive. Avec 12000 tokens, il peut g√©n√©rer aussi bien des rapports standards (15-25 sources) que des rapports approfondis (60 sources) sans limitation.

### Pourquoi ne pas utiliser sonar-reasoning?

**R√©ponse**: sonar-reasoning (16000 tokens) est con√ßu pour des analyses complexes multi-√©tapes avec raisonnement structur√©. Actuellement, nos rapports n'utilisent pas ce niveau de complexit√©. sonar-pro (12000 tokens) est suffisant et plus √©conomique.

### Peut-on changer le mod√®le dynamiquement?

**R√©ponse**: Oui, via les variables d'environnement. Modifier `.env` puis red√©marrer:
```bash
PERPLEXITY_MODEL_ANALYSIS=sonar-reasoning  # Exemple: tester reasoning
docker compose restart backend-service
```

### Les 60 sources fonctionnent avec 12000 tokens?

**R√©ponse**: Oui. Le nombre de sources ne d√©pend pas des tokens mais de la recherche web Perplexity. Les 12000 tokens concernent la longueur du rapport g√©n√©r√© (8000-10000 mots + bibliographie), ce qui est largement suffisant.

### Comment forcer un rapport standard √† utiliser 60 sources?

**R√©ponse**: Inclure "approfondi" dans `analysis_type`:
```json
{
  "analysis_type": "synthese_executive_approfondie"
}
```

---

## R√©f√©rences

- **Documentation Perplexity**: https://docs.perplexity.ai/
- **Mod√®les Sonar**: https://docs.perplexity.ai/docs/model-cards
- **Pricing**: https://www.perplexity.ai/pricing
- **Configuration Backend**: `backend-service/app/main.py` (lignes 43-52, 543-550)
- **Configuration Env**: `env.example` (lignes 17-22)

---

**Derni√®re mise √† jour**: 15 novembre 2025  
**Maintenu par**: √âquipe Insight MVP

